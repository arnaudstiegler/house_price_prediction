{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/lightgbm/__init__.py:45: FutureWarning: Starting from version 2.1.4, the library file in distribution wheels for macOS will be built by the Apple Clang compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you won't need to install the gcc compiler anymore.\n",
      "Instead of that, you'll need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import  GradientBoostingRegressor,RandomForestRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/merged_df.csv')\n",
    "df = df.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = df.drop(['_SALE_PRICE_'],axis=1)\n",
    "y=df['_SALE_PRICE_'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for random forest: 0.645430454884\n"
     ]
    }
   ],
   "source": [
    "RFR=RandomForestRegressor(n_estimators=100, max_depth=5,random_state=0)\n",
    "rmse = np.sqrt(-cross_val_score(RFR, x, y, scoring=\"neg_mean_squared_error\", cv = 3,n_jobs=1))\n",
    "print(\"RMSE for random forest: \" + str(rmse.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for lgbm: 0.713403879191\n"
     ]
    }
   ],
   "source": [
    "LGB = lgb.LGBMRegressor(objective= 'regression',min_data_in_leaf=40,n_estimator=1000,\n",
    "                        num_leaves= 5,metric='mse',learning_rate=0.01,\n",
    "                        bagging_fraction=0.8,feature_fraction=0.8)\n",
    "rmse= np.sqrt(-cross_val_score(LGB, x, y, scoring=\"neg_mean_squared_error\", cv = 3))\n",
    "print(\"RMSE for lgbm: \" + str(rmse.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for GradientBoostingRegressor: 0.718573909773\n"
     ]
    }
   ],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=100, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "rmse = np.sqrt(-cross_val_score(GBoost, x, y, scoring=\"neg_mean_squared_error\", cv = 3))\n",
    "print(\"RMSE for GradientBoostingRegressor: \" + str(rmse.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Memory consumption problem -> Need to much ram because of the number of features\n",
    "#KRR = KernelRidge(alpha=1.0,kernel=\"polynomial\")\n",
    "#score = cross_val_score(KRR, x, y, scoring=\"neg_mean_squared_error\", cv = 3)\n",
    "#rmse= np.sqrt(-score)\n",
    "#print(\"RMSE for KRR: \" + str(rmse.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for SVR: 0.749965690337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel='rbf',max_iter=1500,gamma=0.1)\n",
    "score = cross_val_score(svr, x, y, scoring=\"neg_mean_squared_error\", cv = 3)\n",
    "rmse= np.sqrt(-score)\n",
    "print(\"RMSE for SVR: \" + str(rmse.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for averaged models: 0.675592377337\n"
     ]
    }
   ],
   "source": [
    "#DISCARDING THE LEAST SUCCESSFUL MODEL\n",
    "averaged_models = AveragingModels(models = (LGB, GBoost, RFR))\n",
    "\n",
    "rmse = np.sqrt(-cross_val_score(averaged_models, x, y, scoring=\"neg_mean_squared_error\", cv = 3))\n",
    "print(\"RMSE for averaged models: \" + str(rmse.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Performance Recap\n",
    "Overall, random forest provides the best performance, while both boosted trees algorithms are performing poorly. The SVR might need more iterations and a better parameter tuning to get the best out of it.\n",
    "Even though I used cross validation, I think a higher number of folds would have provided more accurate assessment of each model. However, due to computation time, I restrained the number to 3.\n",
    "I also tried to average models to see if it could actually improve the results of the random forest, but the boosted trees are performing too poorly to help.\n",
    "\n",
    "I would have like to also try a kernel ridge regression, but the memory consumption is way too high for my computer because of the high number of features.\n",
    "\n",
    "Having tested on processing for the dataset, some little tweaks have a big impact on the performance: for instance, keeping the rows that have a year_built = 0 provides great improvements to the results for the boosted trees. However, I tried to have a coherent approach to the dataset, and not tailor it to get the best results to avoid skewing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
